{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromedriver-autoinstaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Can not find chromedriver for currently installed chrome version.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "chromedriver_autoinstaller.install()  # Check if the current version of chromedriver exists\n",
    "                                      # and if it doesn't exist, download it automatically,\n",
    "                                      # then add chromedriver to path\n",
    "\n",
    "# driver = webdriver.Chrome()\n",
    "# driver.get(\"http://www.python.org\")\n",
    "# assert \"Python\" in driver.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "\n",
    "browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "browser.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jwilson1\\GitHub\\ChromaDB_Chatbot_Public\\test.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jwilson1/GitHub/ChromaDB_Chatbot_Public/test.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# Save the HTML content to a .html file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jwilson1/GitHub/ChromaDB_Chatbot_Public/test.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mscraped_content.html\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m html_file:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jwilson1/GitHub/ChromaDB_Chatbot_Public/test.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         html_file\u001b[39m.\u001b[39mwrite(page_source\u001b[39m.\u001b[39;49mhtml)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jwilson1/GitHub/ChromaDB_Chatbot_Public/test.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mContent has been successfully scraped and saved as \u001b[39m\u001b[39m'\u001b[39m\u001b[39mscraped_content.html\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jwilson1/GitHub/ChromaDB_Chatbot_Public/test.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jwilson1/GitHub/ChromaDB_Chatbot_Public/test.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# Close the webdriver\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'html'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = \"https://domo-support.domo.com/s/article/36004740075?language=en_US\"\n",
    "\n",
    "# Set up the Chrome webdriver (you need to have ChromeDriver installed)\n",
    "# chrome_driver_path = \"../../../../Program Files/\"  # Replace with your ChromeDriver executable path\n",
    "# chrome_service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "try:\n",
    "    # Open the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for a few seconds for the page to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Get the page source after it has loaded\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Save the HTML content to a .html file\n",
    "    with open(\"scraped_content.html\", \"w\", encoding=\"utf-8\") as html_file:\n",
    "        html_file.write(page_source)\n",
    "\n",
    "    print(\"Content has been successfully scraped and saved as 'scraped_content.html'.\")\n",
    "\n",
    "finally:\n",
    "    # Close the webdriver\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find main content on the page.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class AsyncWebCrawler:\n",
    "    def __init__(self, start_url):\n",
    "        self.start_url = start_url\n",
    "        self.visited_urls = set()\n",
    "        self.to_crawl = [start_url]\n",
    "\n",
    "    async def fetch_page(self, url, client):\n",
    "        try:\n",
    "            async with client.stream(\"GET\", url) as response:\n",
    "                if response.status_code == 200:\n",
    "                    page_source = await response.aread()\n",
    "                    return url, page_source\n",
    "        except Exception as e:\n",
    "            print(f\"Error while fetching {url}: {str(e}\")\n",
    "        return None\n",
    "\n",
    "    async def crawl(self, max_pages=10):\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            while self.to_crawl and len(self.visited_urls) < max_pages:\n",
    "                tasks = []\n",
    "                for url in self.to_crawl:\n",
    "                    if url not in self.visited_urls:\n",
    "                        tasks.append(self.fetch_page(url, client))\n",
    "                if tasks:\n",
    "                    results = await asyncio.gather(*tasks)\n",
    "                    for url, page_source in results:\n",
    "                        if page_source:\n",
    "                            self.visited_urls.add(url)\n",
    "                            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                            for link in soup.find_all('a', href=True):\n",
    "                                next_url = link['href']\n",
    "                                full_url = urljoin(url, next_url)\n",
    "                                parsed_url = urlparse(full_url)\n",
    "                                if parsed_url.scheme and parsed_url.netloc:\n",
    "                                    self.to_crawl.append(full_url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://example.com\"  # Replace with your starting URL\n",
    "    crawler = AsyncWebCrawler(start_url)\n",
    "    asyncio.run(crawler.crawl(max_pages=10))\n",
    "    print(\"Visited URLs:\")\n",
    "    for url in crawler.visited_urls:\n",
    "        print(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
